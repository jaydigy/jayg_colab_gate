{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyheEkTo29BySw4UjJEbpT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jaydigy/jayg_colab_gate/blob/main/ak_makemore_pt4_by_jayg.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "makemore : becoming a backprop ninja"
      ],
      "metadata": {
        "id": "80AdKgMsFdkM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "acmRS5u-DBmv"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget \"https://raw.githubusercontent.com/jaydigy/jayg_colab_gate/main/names.txt\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4DBg_kGuDvpI",
        "outputId": "9a307304-cc6c-4239-9c9f-ff9dc314caa4"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-24 12:15:41--  https://raw.githubusercontent.com/jaydigy/jayg_colab_gate/main/names.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 228145 (223K) [text/plain]\n",
            "Saving to: ‘names.txt.2’\n",
            "\n",
            "names.txt.2         100%[===================>] 222.80K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-05-24 12:15:41 (22.6 MB/s) - ‘names.txt.2’ saved [228145/228145]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#read all the words\n",
        "words = open('names.txt', 'r').read().splitlines()\n",
        "print(words[:8])\n",
        "print(len(words))\n",
        "print(max(len(w) for w in words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXKehOAHFuEi",
        "outputId": "f918a5f7-00a6-4edc-8c22-cb5ce6991be8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']\n",
            "32033\n",
            "15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build the vocabulary of characters and mapping to/from integers\n",
        "chars = sorted(list(set(''.join(words))))\n",
        "stoi = {s:i+1 for i, s in enumerate(chars)}\n",
        "stoi['.'] = 0\n",
        "itos = {i:s for s, i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print(itos)\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-7dATMyWGFWR",
        "outputId": "559d5975-26df-4413-9ed2-af7765e22a63"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n",
            "27\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#build the dataset\n",
        "block_size = 3 #context length : how many chracters do we take to predict the next one?\n",
        "\n",
        "def build_dataset(words):\n",
        "    X, Y = [], []\n",
        "\n",
        "    for w in words:\n",
        "        context = [0] * block_size\n",
        "        for ch in w + '.':\n",
        "            ix = stoi[ch]\n",
        "            X.append(context)\n",
        "            Y.append(ix)\n",
        "            context = context[1:] + [ix] #crop and append\n",
        "\n",
        "    X = torch.tensor(X)\n",
        "    Y = torch.tensor(Y)\n",
        "    print(X.shape, Y.shape)\n",
        "    return X, Y\n",
        "\n",
        "import random\n",
        "random.seed(42)\n",
        "random.shuffle(words)\n",
        "n1 = int(0.8*len(words))\n",
        "n2 = int(0.9*len(words))\n",
        "\n",
        "Xtr, Ytr = build_dataset(words[:n1])        #80%\n",
        "Xdev, Ydev = build_dataset(words[n1:n2])    #10%\n",
        "Xte, Yte = build_dataset(words[n2:])        #10%"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zsmb_E4QGFTV",
        "outputId": "c5a41aef-7d99-4c01-e96d-a1f1fab7291e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([182625, 3]) torch.Size([182625])\n",
            "torch.Size([22655, 3]) torch.Size([22655])\n",
            "torch.Size([22866, 3]) torch.Size([22866])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ok biolerplate done, now we get to the action :"
      ],
      "metadata": {
        "id": "Y3R5m5mGGZfz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# utility function we will use later when comparing manual gradients to PyTorch gradients\n",
        "def cmp(s, dt, t) :\n",
        "    ex = torch.all(dt == t.grad).item()\n",
        "    app = torch.allclose(dt, t.grad)\n",
        "    maxdiff = (dt - t.grad).abs().max().item()\n",
        "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')"
      ],
      "metadata": {
        "id": "fR0bQMNrGFQP"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "youtube 8:45 below code check"
      ],
      "metadata": {
        "id": "u7H831SGIhox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP revisited \n",
        "n_embd = 10 #the dimensinaliity of the character embedding vectors\n",
        "n_hidden = 200 # the number of the neuron in the hidden layer of mlp\n",
        "\n",
        "g = torch.Generator().manual_seed(214783647) # for reproducibility\n",
        "C = torch.randn(vocab_size, n_embd,             generator=g)\n",
        "# layer 1\n",
        "W1 = torch.randn((n_embd*block_size, n_hidden), generator=g) * (5/3)/((n_embd*block_size)**0.5) # * 0.2 의 토치 kaiser normal 기능구현함\n",
        "b1 = torch.randn(n_hidden,                      generator=g) * 0.1\n",
        "# layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),        generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                    generator=g) * 0.1\n",
        "\n",
        "#BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1+1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "# Note : I am initializating many of these parameters in non-standard ways\n",
        "# because sometimes initializating with e.g. all zeros could mask an incorrect\n",
        "# implementation of backward pass.\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "    p.requires_grad = True  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gX4AnQuGGFNJ",
        "outputId": "dac058c7-dfee-4f43-8a6a-6202039b9cf9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "n = batch_size # a shorrter variable also, for convenience\n",
        "# construct a minibatch\n",
        "ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y"
      ],
      "metadata": {
        "id": "OfnD7v7wGFJ4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# forward pass, \"chunkated\" into smaller steps that are possible to backward one at a time\n",
        "\n",
        "emb = C[Xb] #embded the characters into vectors\n",
        "embcat = emb.view(emb.shape[0], -1) #concatenate the vectors\n",
        "# linear layer 1\n",
        "hprebn = embcat @ W1 + b1 #hidden layer pre-activation\n",
        "# BatchNorm layer\n",
        "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "bndiff = hprebn - bnmeani\n",
        "bndiff2 = bndiff ** 2\n",
        "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note : Bessel's correction )dividing by n-1, not n)\n",
        "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "bnraw = bndiff * bnvar_inv\n",
        "hpreact = bngain * bnraw + bnbias\n",
        "# Non-linearity\n",
        "h = torch.tanh(hpreact) # hidden layer\n",
        "# Linear layer 2\n",
        "logits = h @ W2 + b2 # output layer\n",
        "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
        "logit_maxes = logits.max(1, keepdim=True).values\n",
        "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "counts = norm_logits.exp()\n",
        "counts_sum = counts.sum(1, keepdims=True)\n",
        "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backdrop to be bit exact...\n",
        "probs = counts * counts_sum_inv\n",
        "logprobs = probs.log()\n",
        "loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# PyTorch backward pass\n",
        "for p in parameters:\n",
        "    p.grad = None\n",
        "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
        "        norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
        "        bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
        "        embcat, emb]:\n",
        "    t.retain_grad()\n",
        "loss.backward()\n",
        "loss"
      ],
      "metadata": {
        "id": "JbjOlLwDGFGu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c52d9ea-6040-4778-cda5-3544441fa11d"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3.3446, grad_fn=<NegBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X2UZBgVgm2E",
        "outputId": "d187ebc3-3275-484d-b30f-82e014448c79"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "logprobs[range(n), Yb].shape\n",
        "#logprobs.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IJADR_f0gkNJ",
        "outputId": "a586649b-0641-45cb-9213-34310046ad2d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([32])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss = -(a+b+c) /3 = -a/3 + -b/3 + -c/3\n",
        "# dloss/da = -1/3"
      ],
      "metadata": {
        "id": "OicVh3atpNAx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts.shape, counts_sum_inv.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3AiTNWOfstZ",
        "outputId": "1317c306-c2bd-4619-85b2-8f9f77933835"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]), torch.Size([32, 1]))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# dh = dlogits @ W2.T -> 아래의 텐서 특성을 보고 파악해도 된다. \n",
        "# dw2 = h.T @ dlogits -> 아래의 텐서 특성을 보고 파악해도 된다. \n",
        "# db2 = dlogits.sum(0)"
      ],
      "metadata": {
        "id": "_0BltH7yojZ1"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits.shape, h.shape, W2.shape, b2.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bh27Fjryl5u7",
        "outputId": "99df4a56-f5ef-4e36-e645-5322102c144e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([32, 27]),\n",
              " torch.Size([32, 200]),\n",
              " torch.Size([200, 27]),\n",
              " torch.Size([27]))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exercise 1 : backprop through the whole thing manually,\n",
        "# backpropagating through exactly all of the variables\n",
        "# as they are defined in the forward pass above, one by one\n",
        "\n",
        "# ---------------\n",
        "# Mein Code GO :)\n",
        "# ---------------\n",
        "dlogprobs = torch.zeros_like(logprobs) # loss = -(a+b+c)/3, dloss/da = -1/3\n",
        "dlogprobs[range(n), Yb] = -1.0/n\n",
        "dprobs = (1.0/probs) * dlogprobs\n",
        "dcounts_sum_inv = (dprobs * counts).sum(1, keepdim=True)\n",
        "dcounts = counts_sum_inv * dprobs\n",
        "dcounts_sum = dcounts_sum_inv * counts_sum**-2 * -1\n",
        "dcounts += torch.ones_like(counts) * dcounts_sum\n",
        "dnorm_logits = counts * dcounts\n",
        "dlogits = dnorm_logits.clone()\n",
        "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
        "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1])*dlogit_maxes\n",
        "dh = dlogits @ W2.T \n",
        "dW2 = h.T @ dlogits \n",
        "db2 = dlogits.sum(0)\n",
        "dhpreact = (1.0 - h**2) * dh\n",
        "dbngain = (bnraw*dhpreact).sum(0, keepdim=True)\n",
        "dbnraw = bngain * dhpreact\n",
        "dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "dbndiff = bnvar_inv * dbnraw\n",
        "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "dbnvar = (-0.5*(bnvar+1e-5)**-1.5) * dbnvar_inv\n",
        "dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2)*dbnvar\n",
        "dbndiff += (2*bndiff) * dbndiff2\n",
        "dhprebn = dbndiff.clone()\n",
        "dbnmeani = (-torch.ones_like(bndiff) * dbndiff).sum(0)\n",
        "dhprebn += 1.0/n * (torch.ones_like(hprebn) * dbnmeani)\n",
        "dembcat = dhprebn @ W1.T\n",
        "dW1 = embcat.T @ dhprebn\n",
        "db1 = dhprebn.sum(0)\n",
        "demb = dembcat.view(emb.shape)\n",
        "dC = torch.zeros_like(C)\n",
        "for k in range(Xb.shape[0]):\n",
        "    for j in range(Xb.shape[1]):\n",
        "        ix = Xb[k,j]\n",
        "        dC[ix] += demb[k,j]\n",
        "\n",
        "cmp('logprobs', dlogprobs, logprobs)\n",
        "cmp('prob', dprobs, probs)\n",
        "cmp('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
        "cmp('counts_sum', dcounts_sum, counts_sum)\n",
        "cmp('dcounts', dcounts, counts)\n",
        "cmp('dnorm_logits', dnorm_logits, norm_logits)\n",
        "cmp('dlogits_max', dlogit_maxes, logit_maxes)\n",
        "cmp('logits', dlogits, logits)\n",
        "cmp('h', dh, h)\n",
        "cmp('w2', dW2, W2)\n",
        "cmp('b2', db2, b2)\n",
        "cmp('hpreact', dhpreact, hpreact)\n",
        "cmp('bngain', dbngain, bngain)\n",
        "cmp('bnbias', dbnbias, bnbias)\n",
        "cmp('bnraw', dbnraw, bnraw)\n",
        "cmp('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
        "cmp('bnvar', dbnvar, bnvar)\n",
        "cmp('bndiff2', dbndiff2, bndiff2)\n",
        "cmp('bndiff', dbndiff, bndiff)\n",
        "cmp('bnmeani', dbnmeani, bnmeani)\n",
        "cmp('hprebn', dhprebn, hprebn)\n",
        "cmp('emb', demb, emb)\n",
        "cmp('dC', dC, C)\n"
      ],
      "metadata": {
        "id": "4UcBSszvGFDC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "29f79bdb-ed41-4bfd-83e3-993e011d9d54"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "prob            | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "dcounts         | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "dnorm_logits    | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "dlogits_max     | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "w2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
            "hpreact         | exact: False | approximate: True  | maxdiff: 4.656612873077393e-10\n",
            "bngain          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnbias          | exact: False | approximate: True  | maxdiff: 1.862645149230957e-09\n",
            "bnraw           | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
            "bnvar_inv       | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-10\n",
            "bnvar           | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
            "bndiff2         | exact: False | approximate: True  | maxdiff: 1.8189894035458565e-12\n",
            "bndiff          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
            "bnmeani         | exact: False | approximate: True  | maxdiff: 3.4924596548080444e-10\n",
            "hprebn          | exact: False | approximate: True  | maxdiff: 5.820766091346741e-11\n",
            "emb             | exact: False | approximate: True  | maxdiff: 2.3283064365386963e-10\n",
            "dC              | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 2 : backprop through cross_entropy but all in one go\n",
        "# to complete this challenge look at the mathematical expression of the loss,\n",
        "# take the derivative, simplify the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "# before :\n",
        "# logit_maxes = logits.max(1, keepdim=True).values\n",
        "# norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
        "# counts = norm_logits.exp()\n",
        "# counts_sum = counts.sum(1, keepdims=True)\n",
        "# counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backdrop to be bit exact...\n",
        "# probs = counts * counts_sum_inv\n",
        "# logprobs = probs.log()\n",
        "# loss = -logprobs[range(n), Yb].mean()\n",
        "\n",
        "# now :\n",
        "loss_fast = F.cross_entropy(logits, Yb)\n",
        "print(loss_fast.item(), 'diff:', (loss_fast-loss).item())\n",
        "\n"
      ],
      "metadata": {
        "id": "pNNLGkZGGE-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0034a33-0d55-4105-e8d1-15eb5e0f361c"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3.344575881958008 diff: 2.384185791015625e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "dlogits = F.softmax(logits, 1)\n",
        "dlogits[range(n), Yb] -= 1\n",
        "dlogits /= n\n",
        "\n",
        "cmp('logits', dlogits, logits) # I can only get approximate to be true. max diff is not so big"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9xXSUG-9Evi",
        "outputId": "bea9518e-c7f7-45e0-cb28-ca5fb786665c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "logits          | exact: False | approximate: True  | maxdiff: 7.683411240577698e-09\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "F.softmax(logits,1)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rPULCmAX_HMi",
        "outputId": "f00bd225-598d-49fa-e1d3-040fb60fd4ba"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0403, 0.0348, 0.0357, 0.0275, 0.0462, 0.0341, 0.0333, 0.0532, 0.0377,\n",
              "        0.0300, 0.0507, 0.0307, 0.0382, 0.0381, 0.0288, 0.0481, 0.0258, 0.0368,\n",
              "        0.0385, 0.0322, 0.0400, 0.0462, 0.0440, 0.0292, 0.0299, 0.0352, 0.0346],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dlogits[0]*n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6b32492-pQb",
        "outputId": "3d93bda0-f086-41c2-f3a8-51f591246a46"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 0.0403, -0.9652,  0.0357,  0.0275,  0.0462,  0.0341,  0.0333,  0.0532,\n",
              "         0.0377,  0.0300,  0.0507,  0.0307,  0.0382,  0.0381,  0.0288,  0.0481,\n",
              "         0.0258,  0.0368,  0.0385,  0.0322,  0.0400,  0.0462,  0.0440,  0.0292,\n",
              "         0.0299,  0.0352,  0.0346], grad_fn=<MulBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(8,8))\n",
        "plt.imshow(dlogits.detach(), cmap='gray')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 693
        },
        "id": "IvMX6mZh-tIR",
        "outputId": "8186ce6e-2275-4304-e0cc-ccb15e3dd93e"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fd28d250c10>"
            ]
          },
          "metadata": {},
          "execution_count": 69
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise 3 : backprop through batchnorm but all in one go\n",
        "# to complete this challenge loo at the mathematical expression of the output of batchnorm,\n",
        "# take the derivatibve w.r.t. its input, simple the expression, and just write it out\n",
        "\n",
        "# forward pass\n",
        "\n",
        "# before:\n",
        "# bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
        "# bndiff = hprebn - bnmeani\n",
        "# bndiff2 = bndiff ** 2\n",
        "# bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note : Bessel's correction )dividing by n-1, not n)\n",
        "# bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "# bnraw = bndiff * bnvar_inv\n",
        "# hpreact = bngain * bnraw + bnbias\n",
        "\n",
        "# now:\n",
        "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True))\n",
        "print('max diff:', (hpreact_fast - hpreact).abs().max())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP9NTh6aANwk",
        "outputId": "93388e29-9309-49a8-ca10-5e9ab609412a"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max diff: tensor(0.2534, grad_fn=<MaxBackward1>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward pass\n",
        "\n",
        "# before we had:\n",
        "# dbnraw = bngain * dhpreact\n",
        "# dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "# dbndiff = bnvar_inv * dbnraw\n",
        "# dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
        "# dbnvar = (-0.5*(bnvar+1e-5)**-1.5) * dbnvar_inv\n",
        "# dbndiff2 = (1.0/(n-1))*torch.ones_like(bndiff2)*dbnvar\n",
        "# dbndiff += (2*bndiff) * dbndiff2\n",
        "# dhprebn = dbndiff.clone()\n",
        "\n",
        "# calculate dhprebn given dhpreact (i.e. backprop through the batchnorm)\n",
        "# (you'll also need to use some fo the variables from the forward pass up above)\n",
        "dhprebn = bngain*bnvar_inv/n * (n * dhpreact - dhpreact.sum(0) - n/(n-1) * bnraw * (dhpreact*bnraw).sum(0))\n",
        "\n",
        "cmp('hprebn', dhprebn, hprebn) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85sQ5a4UCAFW",
        "outputId": "8cee2b2e-5b6c-439e-9c7d-ecafcba969ea"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hprebn          | exact: False | approximate: True  | maxdiff: 4.0745362639427185e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exercise 4  : putting it all together :\n",
        "# train the MLP neural net with your own backward pass\n",
        "\n",
        "# init \n",
        "n_embd = 10 #the dimensinaliity of the character embedding vectors\n",
        "n_hidden = 200 # the number of the neuron in the hidden layer of mlp\n",
        "\n",
        "g = torch.Generator().manual_seed(214783647) # for reproducibility\n",
        "C = torch.randn(vocab_size, n_embd,             generator=g)\n",
        "# layer 1\n",
        "W1 = torch.randn((n_embd*block_size, n_hidden), generator=g) * (5/3)/((n_embd*block_size)**0.5) # * 0.2 의 토치 kaiser normal 기능구현함\n",
        "b1 = torch.randn(n_hidden,                      generator=g) * 0.1\n",
        "# layer 2\n",
        "W2 = torch.randn((n_hidden, vocab_size),        generator=g) * 0.1\n",
        "b2 = torch.randn(vocab_size,                    generator=g) * 0.1\n",
        "\n",
        "#BatchNorm parameters\n",
        "bngain = torch.randn((1, n_hidden))*0.1+1.0\n",
        "bnbias = torch.randn((1, n_hidden))*0.1\n",
        "\n",
        "parameters = [C, W1, b1, W2, b2, bngain, bnbias]\n",
        "print(sum(p.nelement() for p in parameters)) # number of parameters in total\n",
        "for p in parameters:\n",
        "    p.requires_grad = True  \n",
        "\n",
        "# same optimization as last time\n",
        "max_steps = 200000\n",
        "batch_size = 32\n",
        "n = batch_size # convenience\n",
        "lossi = []\n",
        "\n",
        "# use this context manager for efficiency once your backward pass is written (TODO)\n",
        "with torch.no_grad() :\n",
        "\n",
        "    # kick off optimization\n",
        "    for i in range(max_steps):\n",
        "\n",
        "        #minibatch construct\n",
        "        ix = torch.randint(0, Xtr.shape[0], (batch_size,), generator=g)\n",
        "        Xb, Yb = Xtr[ix], Ytr[ix] # batch X, Y\n",
        "\n",
        "        #forward pass    \n",
        "        emb = C[Xb] #embded the characters into vectors\n",
        "        embcat = emb.view(emb.shape[0], -1) #concatenate the vectors\n",
        "        # linear layer \n",
        "        hprebn = embcat @ W1 + b1 #hidden layer pre-activation\n",
        "        # BatchNorm layer\n",
        "        bnmean = hprebn.mean(0, keepdim=True)\n",
        "        # bndiff = hprebn - bnmeani\n",
        "        # bndiff2 = bndiff ** 2\n",
        "        bnvar = hprebn.var(0, keepdim=True, unbiased=True) # note : Bessel's correction )dividing by n-1, not n)\n",
        "        bnvar_inv = (bnvar + 1e-5)**-0.5\n",
        "        bnraw = (hprebn-bnmean) * bnvar_inv\n",
        "        hpreact = bngain * bnraw + bnbias\n",
        "        # Non-linearity\n",
        "        h = torch.tanh(hpreact) # hidden layer\n",
        "        # Linear layer 2\n",
        "        logits = h @ W2 + b2 # output layer\n",
        "        loss = F.cross_entropy(logits, Yb) #loss function\n",
        "\n",
        "        # PyTorch backward pass\n",
        "        for p in parameters:\n",
        "            p.grad = None\n",
        "        #loss.backward()\n",
        "\n",
        "        # manual backprop!\n",
        "        dlogits = F.softmax(logits, 1)\n",
        "        dlogits[range(n), Yb] -= 1\n",
        "        dlogits /= n\n",
        "        # 2nd layer backprop\n",
        "        dh = dlogits @ W2.T \n",
        "        dW2 = h.T @ dlogits \n",
        "        db2 = dlogits.sum(0)\n",
        "        # tanh\n",
        "        dhpreact = (1.0 - h**2) * dh\n",
        "        # bathnorm backprop\n",
        "        dbngain = (bnraw*dhpreact).sum(0, keepdim=True)\n",
        "        dbnbias = dhpreact.sum(0, keepdim=True)\n",
        "        dhprebn = bngain*bnvar_inv/n * (n * dhpreact - dhpreact.sum(0) - n/(n-1) * bnraw * (dhpreact*bnraw).sum(0))\n",
        "        # 1st layer\n",
        "        dembcat = dhprebn @ W1.T\n",
        "        dW1 = embcat.T @ dhprebn\n",
        "        db1 = dhprebn.sum(0)\n",
        "        # embedding\n",
        "        demb = dembcat.view(emb.shape)\n",
        "        dC = torch.zeros_like(C)\n",
        "        for k in range(Xb.shape[0]):\n",
        "            for j in range(Xb.shape[1]):\n",
        "                ix = Xb[k,j]\n",
        "                dC[ix] += demb[k,j]\n",
        "        grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
        "        #--------------------\n",
        "\n",
        "        # update\n",
        "        lr = 0.1 if i < 100000 else 0.01 # step learning rate decay\n",
        "        for p, grad in zip(parameters, grads):\n",
        "            #p.data += -lr * p.grad # old way of cheems doge (using PyTorch grad from .backward())\n",
        "            p.data += -lr * grad # new way of swole doge TODO: enable\n",
        "\n",
        "        # track stats\n",
        "        if i % 10000 == 0: # print every once in a while\n",
        "            print(f'{i:7d}/{max_steps:7d}: {loss.item():.4f}')\n",
        "        lossi.append(loss.log10().item())\n",
        "\n",
        "    #   if i >= 100: # TODO: delete early breaking when you're ready to train the full net\n",
        "    #     break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqBWIWdvFgVN",
        "outputId": "473d1a5c-3d86-4738-f232-8d32d5656926"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12297\n",
            "      0/ 200000: 3.5949\n",
            "  10000/ 200000: 2.2800\n",
            "  20000/ 200000: 2.1599\n",
            "  30000/ 200000: 1.9092\n",
            "  40000/ 200000: 2.0792\n",
            "  50000/ 200000: 2.2160\n",
            "  60000/ 200000: 2.3040\n",
            "  70000/ 200000: 1.9227\n",
            "  80000/ 200000: 2.1023\n",
            "  90000/ 200000: 1.7679\n",
            " 100000/ 200000: 1.8332\n",
            " 110000/ 200000: 2.2940\n",
            " 120000/ 200000: 2.4554\n",
            " 130000/ 200000: 2.0557\n",
            " 140000/ 200000: 2.4813\n",
            " 150000/ 200000: 2.2429\n",
            " 160000/ 200000: 1.6935\n",
            " 170000/ 200000: 2.0374\n",
            " 180000/ 200000: 1.7214\n",
            " 190000/ 200000: 1.9209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# useful for checking your gradients\n",
        "# for p,g in zip(parameters, grads):\n",
        "#   cmp(str(tuple(p.shape)), g, p)"
      ],
      "metadata": {
        "id": "k2ptPKg_NE4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calibrate the batch norm at the end of training\n",
        "\n",
        "with torch.no_grad():\n",
        "  # pass the training set through\n",
        "  emb = C[Xtr]\n",
        "  embcat = emb.view(emb.shape[0], -1)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  # measure the mean/std over the entire training set\n",
        "  bnmean = hpreact.mean(0, keepdim=True)\n",
        "  bnvar = hpreact.var(0, keepdim=True, unbiased=True)"
      ],
      "metadata": {
        "id": "A-rEAwBgNF5Z"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate train and val loss\n",
        "\n",
        "@torch.no_grad() # this decorator disables gradient tracking\n",
        "def split_loss(split):\n",
        "  x,y = {\n",
        "    'train': (Xtr, Ytr),\n",
        "    'val': (Xdev, Ydev),\n",
        "    'test': (Xte, Yte),\n",
        "  }[split]\n",
        "  emb = C[x] # (N, block_size, n_embd)\n",
        "  embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "  hpreact = embcat @ W1 + b1\n",
        "  hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "  h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "  logits = h @ W2 + b2 # (N, vocab_size)\n",
        "  loss = F.cross_entropy(logits, y)\n",
        "  print(split, loss.item())\n",
        "\n",
        "split_loss('train')\n",
        "split_loss('val')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iaFZeWaWNJov",
        "outputId": "06bdfa82-5847-4c8f-c9ad-b94ebf174c9a"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train 2.070340633392334\n",
            "val 2.112194061279297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sample from the model\n",
        "g = torch.Generator().manual_seed(2147483647 + 10)\n",
        "\n",
        "for _ in range(20):\n",
        "    \n",
        "    out = []\n",
        "    context = [0] * block_size # initialize with all ...\n",
        "    while True:\n",
        "      # ------------\n",
        "      # forward pass:\n",
        "      # Embedding\n",
        "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
        "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
        "      hpreact = embcat @ W1 + b1\n",
        "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
        "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
        "      logits = h @ W2 + b2 # (N, vocab_size)\n",
        "      # ------------\n",
        "      # Sample\n",
        "      probs = F.softmax(logits, dim=1)\n",
        "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
        "      context = context[1:] + [ix]\n",
        "      out.append(ix)\n",
        "      if ix == 0:\n",
        "        break\n",
        "    \n",
        "    print(''.join(itos[i] for i in out))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uSEuildQNNDv",
        "outputId": "029c99fe-b4c3-4d56-9063-c3ad1333c9e4"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "montaimyah.\n",
            "see.\n",
            "madhayla.\n",
            "reimani.\n",
            "jarlee.\n",
            "adelynnelin.\n",
            "shy.\n",
            "jenne.\n",
            "elies.\n",
            "anar.\n",
            "katzion.\n",
            "kalin.\n",
            "shabergiaghanst.\n",
            "jair.\n",
            "jennex.\n",
            "terian.\n",
            "brence.\n",
            "ryyah.\n",
            "faeha.\n",
            "kayshayan.\n"
          ]
        }
      ]
    }
  ]
}